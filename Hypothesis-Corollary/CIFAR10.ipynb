{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pickle\n",
    "# 加载数据集\n",
    "def load_cifar10_batch(filename):\n",
    "  \n",
    "    with open(filename,'rb') as f:\n",
    "        datadict=pickle.load(f,encoding='latin1')\n",
    "        X=datadict['data']\n",
    "        Y=datadict['labels']\n",
    "        Y=np.array(Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(root):\n",
    "    xs=[]\n",
    "    ys=[]\n",
    "    for b in range(1,6):\n",
    "        f=os.path.join(root,'data_batch_%d'%(b,))\n",
    "        X,Y=load_cifar10_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr= np.vstack(xs)\n",
    "    Ytr=np.hstack(ys)\n",
    "    del X,Y\n",
    "    Xte,Yte=load_cifar10_batch(os.path.join(root,'test_batch'))\n",
    "    return Xtr,Ytr,Xte,Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr,Ytr,Xte,Yte=load_cifar10(\"./cifar10/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000,) (10000, 3072) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape, Ytr.shape,Xte.shape,Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((Xtr,Xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.concatenate((Ytr, Yte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3072) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 3072)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基分类器1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26761111111111113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "start_time_base1=time.time()\n",
    "classifier1=DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=18,min_samples_split=5,random_state=42)#第一次测试最优\n",
    "classifier1.fit(X_train,y_train)\n",
    "score1=classifier1.score(X_test,y_test)\n",
    "print(score1)\n",
    "end_time_base1=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基分类器所用时间： 195.43886256217957\n"
     ]
    }
   ],
   "source": [
    "base_time1=end_time_base1-start_time_base1\n",
    "print('基分类器所用时间：',base_time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_cifar10/base1_learner_cifar10.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "#保存Model\n",
    "joblib.dump(classifier1,'model_cifar10/base1_learner_cifar10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6970195770263672\n"
     ]
    }
   ],
   "source": [
    "print(os.path.getsize('model_cifar10/base1_learner_cifar10.pkl')/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_size(target_dir):\n",
    "    pkl_size=[] #MB\n",
    "    dir_list=os.listdir(target_dir)\n",
    "    print(dir_list)\n",
    "    #计算每个文件的大小\n",
    "    for file in dir_list:\n",
    "        file = os.path.join(target_dir, file)\n",
    "        #如果是文件，直接通过getsize计算大小并加到size中\n",
    "        if os.path.isfile(file):\n",
    "            pkl_size.append(os.path.getsize(file)/1024/1024) #MB\n",
    "    return pkl_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def bagging_clf(base_learner,target_dir,n):\n",
    "    #性能指标\n",
    "    time_bagging=[] #s\n",
    "    top1_bagging=[]\n",
    "    \n",
    "#     n= [20,40,60,80,100,120,140,170,300,400,500]\n",
    "    \n",
    "    for i in tqdm(n):\n",
    "    \n",
    "        start_time_bagging=time.time()\n",
    "\n",
    "        # 创建Bagging集成学习器\n",
    "        bagging_clf = BaggingClassifier(base_estimator=base_learner, n_estimators=i, random_state=42,n_jobs=-1,bootstrap=True)\n",
    "        bagging_clf.fit(X_train, y_train)\n",
    "        y_pred_bagging = bagging_clf.predict(X_test)\n",
    "\n",
    "        # 评估性能\n",
    "    #     print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bagging))\n",
    "        top1_bagging.append(accuracy_score(y_test, y_pred_bagging))\n",
    "\n",
    "        end_time_bagging=time.time()\n",
    "        bagging_time=end_time_bagging-start_time_bagging\n",
    "    #     print('Bagging所用时间：',bagging_time)\n",
    "        time_bagging.append(bagging_time)\n",
    "\n",
    "        joblib.dump(bagging_clf,f'{target_dir}/bagging_cifar10_{i}.pkl')\n",
    "    \n",
    "    return time_bagging,top1_bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 11/11 [6:06:45<00:00, 2000.50s/it]\n"
     ]
    }
   ],
   "source": [
    "time_bagging1,top1_bagging1=bagging_clf(classifier1,'model_cifar10/model1_bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bagging_cifar10_100.pkl', 'bagging_cifar10_120.pkl', 'bagging_cifar10_140.pkl', 'bagging_cifar10_170.pkl', 'bagging_cifar10_20.pkl', 'bagging_cifar10_300.pkl', 'bagging_cifar10_40.pkl', 'bagging_cifar10_400.pkl', 'bagging_cifar10_500.pkl', 'bagging_cifar10_60.pkl', 'bagging_cifar10_80.pkl']\n"
     ]
    }
   ],
   "source": [
    "pkl_size_bagging1=get_dir_size('model_cifar10/model1_bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    基分类器个数  performance           时间  pkl_size_bagging1\n",
      "0       20     0.411222   288.130097         123.077250\n",
      "1       40     0.435889   539.409804         147.316343\n",
      "2       60     0.444333   722.692421         171.655815\n",
      "3       80     0.452333   997.036239         208.376822\n",
      "4      100     0.457722  1210.448237          26.048515\n",
      "5      120     0.461444  1403.024829         365.652562\n",
      "6      140     0.464056  1670.265264          50.342065\n",
      "7      170     0.467111  2067.906025         486.981323\n",
      "8      300     0.469667  3382.287407         608.567158\n",
      "9      400     0.471944  4187.489450          74.638499\n",
      "10     500     0.470833  5526.038850          98.882264\n"
     ]
    }
   ],
   "source": [
    "#保存结果\n",
    "import pandas as pd\n",
    "n= [20,40,60,80,100,120,140,170,300,400,500]\n",
    "c1={\"基分类器个数\" : n,\n",
    "   \"performance\" : top1_bagging1,\n",
    "  \"时间\":time_bagging1,\n",
    "  \"pkl_size_bagging1\":pkl_size_bagging1}#将列表a，b转换成字典\n",
    "bagging_result1=pd.DataFrame(c1)#将字典转换成为数据框\n",
    "print(bagging_result1)\n",
    "bagging_result1.to_csv('model_cifar10/bagging_cifar10_result1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def boosting_clf(base_learner,target_dir,n):\n",
    "\n",
    "    #性能指标\n",
    "    time_boosting=[] #s\n",
    "    top1_boosting=[]\n",
    "\n",
    "#     n= [20,40,60,80,100,120,140,170,300,400,500]\n",
    "\n",
    "    for i in tqdm(n):\n",
    "\n",
    "        start_time_boosting=time.time()\n",
    "\n",
    "        # 创建Bagging集成学习器\n",
    "        boosting_clf = AdaBoostClassifier(base_estimator=base_learner, n_estimators=i, random_state=42)\n",
    "        boosting_clf.fit(X_train, y_train)\n",
    "        y_pred_boosting = boosting_clf.predict(X_test)\n",
    "\n",
    "        # 评估性能\n",
    "        top1_boosting.append(accuracy_score(y_test, y_pred_boosting))\n",
    "\n",
    "        end_time_boosting=time.time()\n",
    "        boosting_time=end_time_boosting-start_time_boosting\n",
    "        time_boosting.append(boosting_time)\n",
    "        \n",
    "        joblib.dump(boosting_clf,f'{target_dir}/boosting_cifar10_{i}.pkl')    \n",
    "    \n",
    "    return time_boosting,top1_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████▉                                               | 4/11 [13:00:16<22:45:29, 11704.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8d928e4cd0db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtime_boosting1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtop1_boosting1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboosting_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'model_cifar10/model1_boosting'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-168d53062ecb>\u001b[0m in \u001b[0;36mboosting_clf\u001b[1;34m(base_learner, target_dir)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# 创建Bagging集成学习器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mboosting_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_learner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mboosting_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0my_pred_boosting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboosting_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miboost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;31m# Boosting step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[0;32m    131\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \"\"\"\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \"\"\"\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_boosting1,top1_boosting1=boosting_clf(classifier1,'model_cifar10/model1_boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_boosting1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-f773ae639052>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_boosting1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtop1_boosting1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#中间结果打印不出来\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'time_boosting1' is not defined"
     ]
    }
   ],
   "source": [
    "print(time_boosting1,top1_boosting1) #中间结果打印不出来 函数中断没有return成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boosting_cifar10_20.pkl', 'boosting_cifar10_40.pkl', 'boosting_cifar10_60.pkl', 'boosting_cifar10_80.pkl']\n"
     ]
    }
   ],
   "source": [
    "pkl_size_boosting1=get_dir_size('model_cifar10/model1_boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.617488861083984, 73.30146789550781, 112.44429397583008, 148.91400909423828]\n"
     ]
    }
   ],
   "source": [
    "print(pkl_size_boosting1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top1_boosting1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-2059309e495c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m cb1={\"基分类器个数\" : n,\n\u001b[1;32m----> 5\u001b[1;33m    \u001b[1;34m\"performance\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtop1_boosting1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m   \u001b[1;34m\"时间\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtime_boosting1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \"pkl_size_boosting\":pkl_size_boosting1}#将列表a，b转换成字典\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top1_boosting1' is not defined"
     ]
    }
   ],
   "source": [
    "#保存结果\n",
    "n= [20,40,60,80]\n",
    "import pandas as pd\n",
    "cb1={\"基分类器个数\" : n,\n",
    "   \"performance\" : top1_boosting1,\n",
    "  \"时间\":time_boosting1,\n",
    "  \"pkl_size_boosting\":pkl_size_boosting1}#将列表a，b转换成字典\n",
    "boost_result1=pd.DataFrame(cb1)#将字典转换成为数据框\n",
    "print(boost_result1)\n",
    "boost_result1.to_csv('model_cifar10/boosting_cifar10_result1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续跑剩下的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10/10 [78:55:48<00:00, 28414.89s/it]\n"
     ]
    }
   ],
   "source": [
    "n= [20,40,60,80,100,120,140,170,200,300]\n",
    "time_boosting11,top1_boosting11=boosting_clf(classifier1,'model_cifar10/model1_boosting',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boosting_cifar10_100.pkl', 'boosting_cifar10_120.pkl', 'boosting_cifar10_140.pkl', 'boosting_cifar10_170.pkl', 'boosting_cifar10_20.pkl', 'boosting_cifar10_200.pkl', 'boosting_cifar10_300.pkl', 'boosting_cifar10_40.pkl', 'boosting_cifar10_60.pkl', 'boosting_cifar10_80.pkl']\n"
     ]
    }
   ],
   "source": [
    "pkl_size_boosting11=get_dir_size('model_cifar10/model1_boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   基分类器个数  performance            时间  pkl_size_boosting\n",
      "0      20     0.340056   4498.058643         183.643673\n",
      "1      40     0.382333   8081.870675         219.865662\n",
      "2      60     0.401111  13839.976938         256.570911\n",
      "3      80     0.415111  17208.776704         312.480135\n",
      "4     100     0.424278  21510.124742          36.617489\n",
      "5     120     0.430833  25002.603672         365.217422\n",
      "6     140     0.434000  29890.642432         547.779746\n",
      "7     170     0.439889  35952.013403          73.301468\n",
      "8     200     0.445333  46887.961143         112.444294\n",
      "9     300     0.457389  81268.161003         148.914009\n"
     ]
    }
   ],
   "source": [
    "#保存结果\n",
    "import pandas as pd\n",
    "cb11={\"基分类器个数\" : n,\n",
    "   \"performance\" : top1_boosting11,\n",
    "  \"时间\":time_boosting11,\n",
    "  \"pkl_size_boosting\":pkl_size_boosting11}#将列表a，b转换成字典\n",
    "boost_result11=pd.DataFrame(cb11)#将字典转换成为数据框\n",
    "print(boost_result11)\n",
    "boost_result11.to_csv('model_cifar10/boosting_cifar10_result1_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行结束之后暂停  重跑下列2组实验 已完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基分类器2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29588888888888887\n"
     ]
    }
   ],
   "source": [
    "start_time_base2=time.time()\n",
    "classifier2=DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=10,min_samples_split=5,random_state=42)#第一次测试最优\n",
    "classifier2.fit(X_train,y_train)\n",
    "score2=classifier2.score(X_test,y_test)\n",
    "print(score2)\n",
    "end_time_base2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基分类器所用时间： 175.94180989265442\n"
     ]
    }
   ],
   "source": [
    "base_time2=end_time_base2-start_time_base2\n",
    "print('基分类器所用时间：',base_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_cifar10/base2_learner_cifar10.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "#保存Model\n",
    "joblib.dump(classifier2,'model_cifar10/base2_learner_cifar10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2199993133544922\n"
     ]
    }
   ],
   "source": [
    "print(os.path.getsize('model_cifar10/base2_learner_cifar10.pkl')/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 10/10 [4:05:00<00:00, 1470.06s/it]\n"
     ]
    }
   ],
   "source": [
    "n= [20,40,60,80,100,120,140,170,200,300]\n",
    "time_bagging2,top1_bagging2=bagging_clf(classifier2,'model_cifar10/model2_bagging',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bagging_cifar10_100.pkl', 'bagging_cifar10_120.pkl', 'bagging_cifar10_140.pkl', 'bagging_cifar10_170.pkl', 'bagging_cifar10_20.pkl', 'bagging_cifar10_200.pkl', 'bagging_cifar10_300.pkl', 'bagging_cifar10_40.pkl', 'bagging_cifar10_60.pkl', 'bagging_cifar10_80.pkl']\n"
     ]
    }
   ],
   "source": [
    "pkl_size_bagging2=get_dir_size('model_cifar10/model2_bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   基分类器个数  performance           时间  pkl_size_bagging\n",
      "0      20     0.399056   260.158357         21.516673\n",
      "1      40     0.412444   503.041526         25.735838\n",
      "2      60     0.418000   709.394526         30.015175\n",
      "3      80     0.418111   972.865941         36.441184\n",
      "4     100     0.421278  1216.894387          4.503319\n",
      "5     120     0.424389  1426.272881         42.799499\n",
      "6     140     0.423889  1678.214157         64.020848\n",
      "7     170     0.424833  2026.910001          8.728173\n",
      "8     200     0.425167  2386.292365         13.001307\n",
      "9     300     0.424056  3516.926649         17.237596\n"
     ]
    }
   ],
   "source": [
    "#保存结果\n",
    "import pandas as pd\n",
    "c2={\"基分类器个数\" : n,\n",
    "   \"performance\" : top1_bagging2,\n",
    "  \"时间\":time_bagging2,\n",
    "  \"pkl_size_bagging\":pkl_size_bagging2}#将列表a，b转换成字典\n",
    "bagging_result2=pd.DataFrame(c2)#将字典转换成为数据框\n",
    "print(bagging_result2)\n",
    "bagging_result2.to_csv('model_cifar10/bagging_cifar10_result2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10/10 [59:47:17<00:00, 21523.76s/it]\n"
     ]
    }
   ],
   "source": [
    "n= [20,40,60,80,100,120,140,170,200,300]\n",
    "time_boosting2,top1_boosting2=boosting_clf(classifier2,'model_cifar10/model2_boosting',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boosting_cifar10_100.pkl', 'boosting_cifar10_120.pkl', 'boosting_cifar10_140.pkl', 'boosting_cifar10_170.pkl', 'boosting_cifar10_20.pkl', 'boosting_cifar10_200.pkl', 'boosting_cifar10_300.pkl', 'boosting_cifar10_40.pkl', 'boosting_cifar10_60.pkl', 'boosting_cifar10_80.pkl']\n"
     ]
    }
   ],
   "source": [
    "pkl_size_boosting2=get_dir_size('model_cifar10/model2_boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   基分类器个数  performance            时间  pkl_size_boosting\n",
      "0      20     0.265056   4241.890586          17.523251\n",
      "1      40     0.294000   8249.265976          21.099564\n",
      "2      60     0.314278  11924.379053          24.692219\n",
      "3      80     0.333389  15817.183200          29.969011\n",
      "4     100     0.342111  19972.752807           3.545620\n",
      "5     120     0.354611  23654.145258          35.483932\n",
      "6     140     0.365222  26287.187249          53.382102\n",
      "7     170     0.372833  25946.995061           6.867722\n",
      "8     200     0.382556  31102.209241          10.350391\n",
      "9     300     0.398944  48039.403347          13.965874\n"
     ]
    }
   ],
   "source": [
    "#保存结果\n",
    "import pandas as pd\n",
    "cb2={\"基分类器个数\" : n,\n",
    "   \"performance\" : top1_boosting2,\n",
    "  \"时间\":time_boosting2,\n",
    "  \"pkl_size_boosting\":pkl_size_boosting2}#将列表a，b转换成字典\n",
    "boost_result2=pd.DataFrame(cb2)#将字典转换成为数据框\n",
    "print(boost_result2)\n",
    "boost_result2.to_csv('model_cifar10/boosting_cifar10_result2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基分类器3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2876666666666667\n"
     ]
    }
   ],
   "source": [
    "start_time_base3=time.time()\n",
    "classifier3=DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=8,min_samples_split=5,random_state=42)#第一次测试最优\n",
    "classifier3.fit(X_train,y_train)\n",
    "score3=classifier3.score(X_test,y_test)\n",
    "print(score3)\n",
    "end_time_base3=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基分类器所用时间： 130.3557469844818\n"
     ]
    }
   ],
   "source": [
    "base_time3=end_time_base3-start_time_base3\n",
    "print('基分类器所用时间：',base_time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_cifar10/base3_learner_cifar10.pkl']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "#保存Model\n",
    "joblib.dump(classifier3,'model_cifar10/base3_learner_cifar10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06669425964355469\n"
     ]
    }
   ],
   "source": [
    "print(os.path.getsize('model_cifar10/base3_learner_cifar10.pkl')/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [2:25:45<00:00, 874.58s/it]\n"
     ]
    }
   ],
   "source": [
    "n= [20,40,60,80,100,120,140,170,200,300]\n",
    "time_bagging3,top1_bagging3=bagging_clf(classifier3,'model_cifar10/model3_bagging',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bagging_cifar10_100.pkl', 'bagging_cifar10_120.pkl', 'bagging_cifar10_140.pkl', 'bagging_cifar10_170.pkl', 'bagging_cifar10_20.pkl', 'bagging_cifar10_200.pkl', 'bagging_cifar10_300.pkl', 'bagging_cifar10_40.pkl', 'bagging_cifar10_60.pkl', 'bagging_cifar10_80.pkl']\n"
     ]
    }
   ],
   "source": [
    "pkl_size_bagging3=get_dir_size('model_cifar10/model3_bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   基分类器个数  performance           时间  pkl_size_bagging\n",
      "0      20     0.378444   175.276000          7.702617\n",
      "1      40     0.383556   379.606413          9.226622\n",
      "2      60     0.386556   480.526399         10.753731\n",
      "3      80     0.388278   566.732618         13.046988\n",
      "4     100     0.389500   862.045287          1.600639\n",
      "5     120     0.389889   979.400436         15.330916\n",
      "6     140     0.389222  1058.413802         22.967259\n",
      "7     170     0.388500  1076.414617          3.122811\n",
      "8     200     0.388611  1309.487308          4.652019\n",
      "9     300     0.388889  1855.717403          6.172914\n"
     ]
    }
   ],
   "source": [
    "#保存结果\n",
    "import pandas as pd\n",
    "c3={\"基分类器个数\" : n,\n",
    "   \"performance\" : top1_bagging3,\n",
    "  \"时间\":time_bagging3,\n",
    "  \"pkl_size_bagging\":pkl_size_bagging3}#将列表a，b转换成字典\n",
    "bagging_result3=pd.DataFrame(c3)#将字典转换成为数据框\n",
    "print(bagging_result3)\n",
    "bagging_result3.to_csv('model_cifar10/bagging_cifar10_result3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10/10 [45:00:43<00:00, 16204.40s/it]\n"
     ]
    }
   ],
   "source": [
    "n= [20,40,60,80,100,120,140,170,200,300]\n",
    "time_boosting3,top1_boosting3=boosting_clf(classifier3,'model_cifar10/model3_boosting',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boosting_cifar10_100.pkl', 'boosting_cifar10_120.pkl', 'boosting_cifar10_140.pkl', 'boosting_cifar10_170.pkl', 'boosting_cifar10_20.pkl', 'boosting_cifar10_200.pkl', 'boosting_cifar10_300.pkl', 'boosting_cifar10_40.pkl', 'boosting_cifar10_60.pkl', 'boosting_cifar10_80.pkl']\n"
     ]
    }
   ],
   "source": [
    "pkl_size_boosting3=get_dir_size('model_cifar10/model3_boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   基分类器个数  performance            时间  pkl_size_boosting\n",
      "0      20     0.277222   2438.547966           6.080624\n",
      "1      40     0.270889   4809.793720           7.300812\n",
      "2      60     0.290556   7272.237727           8.518406\n",
      "3      80     0.309278   9808.519388          10.318987\n",
      "4     100     0.321056  12433.583216           1.261089\n",
      "5     120     0.331278  15291.972367          12.167816\n",
      "6     140     0.341667  18069.785604          18.170188\n",
      "7     170     0.348778  23324.323385           2.430954\n",
      "8     200     0.355722  29081.418851           3.637653\n",
      "9     300     0.375111  39512.265176           4.859398\n"
     ]
    }
   ],
   "source": [
    "#保存结果\n",
    "import pandas as pd\n",
    "cb3={\"基分类器个数\" : n,\n",
    "   \"performance\" : top1_boosting3,\n",
    "  \"时间\":time_boosting3,\n",
    "  \"pkl_size_boosting\":pkl_size_boosting3}#将列表a，b转换成字典\n",
    "boost_result3=pd.DataFrame(cb3)#将字典转换成为数据框\n",
    "print(boost_result3)\n",
    "boost_result3.to_csv('model_cifar10/boosting_cifar10_result3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
